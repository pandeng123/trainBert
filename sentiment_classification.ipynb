{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 3\n",
    "themes = {\"动力\", \"价格\", \"内饰\", \"配置\", \"安全性\", \"外观\", \"操控\", \"油耗\", \"空间\", \"舒适性\"}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChnSentiCorp(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        themes = {\"动力\", \"价格\", \"内饰\", \"配置\", \"安全性\", \"外观\", \"操控\", \"油耗\", \"空间\", \"舒适性\"}\n",
    "        Data = {}\n",
    "        theme_sentiment_pattern = re.compile(r'(\\S+?)#(-?\\d+)')  \n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "\n",
    "                matches = theme_sentiment_pattern.findall(line)\n",
    "\n",
    "                if not matches:\n",
    "                    raise ValueError(f\"Line {idx + 1} does not contain any valid theme-label pair: {line}\")\n",
    "\n",
    "                comment = re.sub(theme_sentiment_pattern, \"\", line).strip()\n",
    "\n",
    "                theme_sentiment_pairs = matches  \n",
    "\n",
    "                total_sentiment = sum(int(sentiment) for _, sentiment in theme_sentiment_pairs)\n",
    "\n",
    "                if total_sentiment > 0:\n",
    "                    sentiment_label = 2  \n",
    "                elif total_sentiment < 0:\n",
    "                    sentiment_label = 0  \n",
    "                else:\n",
    "                    sentiment_label = 1  \n",
    "\n",
    "\n",
    "                Data[idx] = {\n",
    "                    'comment': comment.replace(\" \", \"\"),  \n",
    "                    'themes': [theme for theme, _ in theme_sentiment_pairs],  \n",
    "                    'sentiment': sentiment_label  \n",
    "                }\n",
    "\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_data = ChnSentiCorp('data/train.txt')\n",
    "test_data = ChnSentiCorp('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch_samples):\n",
    "    batch_sentences, batch_labels = [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        batch_sentences.append(sample['comment'])\n",
    "        batch_labels.append(int(sample['sentiment']))\n",
    "\n",
    "    batch_inputs = tokenizer(\n",
    "        batch_sentences,  \n",
    "        max_length=max_length,  \n",
    "        padding=True,  \n",
    "        truncation=True,  \n",
    "        return_tensors=\"pt\",  \n",
    "        return_attention_mask=True \n",
    "    )\n",
    "\n",
    "    input_ids = batch_inputs['input_ids']\n",
    "    attention_mask = batch_inputs['attention_mask']\n",
    "\n",
    "    labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': input_ids, \n",
    "        'attention_mask': attention_mask,  \n",
    "        'labels': labels  \n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_train_step = 0\n",
    "total_train_loss = 0.\n",
    "best_f1_score = 0.\n",
    "total_test_loss = 0\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_train_loss, total_train_step):\n",
    "    progress_bar = tqdm(range(len(dataloader)),disable=True)\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_step_num = epoch * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for step, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = {k: v.to(device) for k, v in batch_data.items()}\n",
    "        outputs = model(**batch_data)\n",
    "        loss, _ = outputs\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_step +=1\n",
    "        progress_bar.set_description(f'loss: {total_train_loss / (finish_step_num + step):>7f}')\n",
    "        progress_bar.update(1)\n",
    "        if total_train_step % 100 == 0:\n",
    "            print(\"训练次数:{}，loss:{}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "    return total_train_loss, total_train_step\n",
    "\n",
    "def test_loop(dataloader, model, epoch):\n",
    "    true_labels, predictions = [], []\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch_data in enumerate(dataloader, start=1):\n",
    "            batch_data = {k: v.to(device) for k, v in batch_data.items()}\n",
    "            outputs = model(**batch_data)\n",
    "            loss, logits = outputs\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            true_labels += batch_data['labels'].cpu().numpy().tolist()\n",
    "            predictions += pred.cpu().numpy().tolist()\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    print(\"整体测试集上的Loss:{}\".format(total_test_loss))\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, epoch)\n",
    "\n",
    "    metrics = classification_report(true_labels, predictions, output_dict=True)\n",
    "\n",
    "    pos_p, pos_r, pos_f1 = metrics['2']['precision'], metrics['2']['recall'], metrics['2']['f1-score']  # 正向情感 (类别 2)\n",
    "\n",
    "    neu_p, neu_r, neu_f1= metrics['1']['precision'], metrics['1']['recall'], metrics['1']['f1-score']   # 中性情感 (类别 1)\n",
    "\n",
    "    neg_p, neg_r, neg_f1 = metrics['0']['precision'], metrics['0']['recall'], metrics['0']['f1-score'] # 负向情感 (类别 0)\n",
    "\n",
    "    macro_f1 = metrics['macro avg']['f1-score']\n",
    "    micro_f1 = metrics['weighted avg']['f1-score']\n",
    "    accuracy = metrics['accuracy']\n",
    "    writer.add_scalar(\"test_accuarcy\", accuracy, epoch)\n",
    "\n",
    "    print(f\"Positive (2): Precision: {pos_p * 100:>0.2f} / Recall: {pos_r * 100:>0.2f} / F1: {pos_f1 * 100:>0.2f}\")\n",
    "    print(f\"Neutral (1): Precision: {neu_p * 100:>0.2f} / Recall: {neu_r * 100:>0.2f} / F1: {neu_f1 * 100:>0.2f}\")\n",
    "    print(f\"Negative (0): Precision: {neg_p * 100:>0.2f} / Recall: {neg_r * 100:>0.2f} / F1: {neg_f1 * 100:>0.2f}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:>0.2f}\")\n",
    "    print(f\"Macro-F1: {macro_f1 * 100:>0.2f} / Micro-F1: {micro_f1 * 100:>0.2f}\\n\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num * len(train_dataloader),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "------------------------------\n",
      "训练次数:100，loss:0.39815378189086914\n",
      "训练次数:200，loss:0.7061346769332886\n",
      "训练次数:300，loss:0.35079506039619446\n",
      "训练次数:400，loss:0.7237738966941833\n",
      "训练次数:500，loss:0.49723997712135315\n",
      "训练次数:600，loss:0.479952335357666\n",
      "训练次数:700，loss:0.6700111031532288\n",
      "训练次数:800，loss:0.6364050507545471\n",
      "训练次数:900，loss:0.9160412549972534\n",
      "训练次数:1000，loss:0.4736132025718689\n",
      "训练次数:1100，loss:0.1950397938489914\n",
      "训练次数:1200，loss:0.8391523361206055\n",
      "训练次数:1300，loss:1.1750240325927734\n",
      "训练次数:1400，loss:1.1670982837677002\n",
      "训练次数:1500，loss:0.5805022716522217\n",
      "训练次数:1600，loss:0.8716732263565063\n",
      "训练次数:1700，loss:0.5139517784118652\n",
      "训练次数:1800，loss:0.8265998959541321\n",
      "训练次数:1900，loss:0.41246238350868225\n",
      "训练次数:2000，loss:0.46494239568710327\n",
      "整体测试集上的Loss:438.3846437856555\n",
      "Positive (2): Precision: 58.22 / Recall: 32.89 / F1: 42.03\n",
      "Neutral (1): Precision: 74.54 / Recall: 92.54 / F1: 82.57\n",
      "Negative (0): Precision: 60.11 / Recall: 23.57 / F1: 33.86\n",
      "Accuracy: 72.26\n",
      "Macro-F1: 52.82 / Micro-F1: 68.47\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/3\n",
      "------------------------------\n",
      "训练次数:2100，loss:0.6439011096954346\n",
      "训练次数:2200，loss:0.3527393341064453\n",
      "训练次数:2300，loss:0.07976146042346954\n",
      "训练次数:2400，loss:0.4666355550289154\n",
      "训练次数:2500，loss:0.769644558429718\n",
      "训练次数:2600，loss:0.38573116064071655\n",
      "训练次数:2700，loss:0.39025139808654785\n",
      "训练次数:2800，loss:0.8855032920837402\n",
      "训练次数:2900，loss:0.5564773678779602\n",
      "训练次数:3000，loss:0.46374714374542236\n",
      "训练次数:3100，loss:2.5351173877716064\n",
      "训练次数:3200，loss:0.5870766043663025\n",
      "训练次数:3300，loss:0.5642117261886597\n",
      "训练次数:3400，loss:0.14938420057296753\n",
      "训练次数:3500，loss:1.3592643737792969\n",
      "训练次数:3600，loss:0.352886825799942\n",
      "训练次数:3700，loss:0.1057986468076706\n",
      "训练次数:3800，loss:1.4600868225097656\n",
      "训练次数:3900，loss:0.7607231736183167\n",
      "训练次数:4000，loss:0.7108713388442993\n",
      "整体测试集上的Loss:448.26220888644457\n",
      "Positive (2): Precision: 51.85 / Recall: 51.99 / F1: 51.92\n",
      "Neutral (1): Precision: 79.17 / Recall: 82.60 / F1: 80.85\n",
      "Negative (0): Precision: 52.14 / Recall: 42.95 / F1: 47.10\n",
      "Accuracy: 71.47\n",
      "Macro-F1: 59.96 / Micro-F1: 70.96\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 3/3\n",
      "------------------------------\n",
      "训练次数:4100，loss:0.4432621896266937\n",
      "训练次数:4200，loss:0.18596211075782776\n",
      "训练次数:4300，loss:0.42319533228874207\n",
      "训练次数:4400，loss:0.046631984412670135\n",
      "训练次数:4500，loss:0.4645426571369171\n",
      "训练次数:4600，loss:0.12507306039333344\n",
      "训练次数:4700，loss:0.04981683939695358\n",
      "训练次数:4800，loss:0.8560562133789062\n",
      "训练次数:4900，loss:0.1138693317770958\n",
      "训练次数:5000，loss:0.1765734851360321\n",
      "训练次数:5100，loss:0.2140653431415558\n",
      "训练次数:5200，loss:0.385734498500824\n",
      "训练次数:5300，loss:0.09752265363931656\n",
      "训练次数:5400，loss:1.0179190635681152\n",
      "训练次数:5500，loss:0.6144363880157471\n",
      "训练次数:5600，loss:0.6734458208084106\n",
      "训练次数:5700，loss:0.11277714371681213\n",
      "训练次数:5800，loss:0.6400430798530579\n",
      "训练次数:5900，loss:0.3110778033733368\n",
      "训练次数:6000，loss:0.1784333884716034\n",
      "整体测试集上的Loss:493.91615619976074\n",
      "Positive (2): Precision: 53.11 / Recall: 49.87 / F1: 51.44\n",
      "Neutral (1): Precision: 79.18 / Recall: 83.26 / F1: 81.17\n",
      "Negative (0): Precision: 53.52 / Recall: 45.15 / F1: 48.98\n",
      "Accuracy: 71.99\n",
      "Macro-F1: 60.53 / Micro-F1: 71.43\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='themes_classification_logs' + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch {epoch + 1}/{epoch_num}\\n\" + 30 * \"-\")\n",
    "    total_train_loss, total_train_step= train_loop(train_dataloader, model, optimizer, lr_scheduler, epoch, total_train_loss, total_train_step)\n",
    "    valid_scores = test_loop(test_dataloader, model, epoch)\n",
    "    macro_f1, micro_f1 = valid_scores['macro avg']['f1-score'], valid_scores['weighted avg']['f1-score']\n",
    "    f1_score = (macro_f1 + micro_f1) / 2\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f'epoch_{epoch + 1}_valid_macrof1_{(macro_f1 * 100):0.3f}_microf1_{(micro_f1 * 100):0.3f}_model_weights.bin'\n",
    "        )\n",
    "\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
