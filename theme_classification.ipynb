{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import os\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "batch_size = 4\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 3\n",
    "themes = {\"动力\", \"价格\", \"内饰\", \"配置\", \"安全性\", \"外观\", \"操控\", \"油耗\", \"空间\", \"舒适性\"}\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(12)\n",
    "\n",
    "def Ac(y_true, y_pred):\n",
    "    return metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "def predict(outputs, alpha=0.4):\n",
    "    predic = torch.sigmoid(outputs)\n",
    "    zero = torch.zeros_like(predic)\n",
    "    topk = torch.topk(predic, k=2, dim=1, largest=True)[1]\n",
    "    for i, x in enumerate(topk):\n",
    "        for y in x:\n",
    "            if predic[i][y] > alpha:\n",
    "                zero[i][y] = 1\n",
    "    return zero.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 8000\n",
      "test set size: 2653\n",
      "{'comment': '因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。', 'themes': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'sentiment': 1}\n"
     ]
    }
   ],
   "source": [
    "class ChnSentiCorp(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.themes = [\"动力\", \"价格\", \"内饰\", \"配置\", \"安全性\", \"外观\", \"操控\", \"油耗\", \"空间\", \"舒适性\"]\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        theme_sentiment_pattern = re.compile(r'(\\S+?)#(-?\\d+)')\n",
    "        Data = {}\n",
    "\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                matches = theme_sentiment_pattern.findall(line)\n",
    "                comment = re.sub(theme_sentiment_pattern, \"\", line).strip()\n",
    "                themes_in_line = [theme for theme, _ in matches]\n",
    "                multi_hot_vector = [1 if theme in themes_in_line else 0 for theme in self.themes]\n",
    "                total_sentiment = sum(int(sentiment) for _, sentiment in matches)\n",
    "\n",
    "                if total_sentiment > 0:\n",
    "                    sentiment_label = 2  \n",
    "                elif total_sentiment < 0:\n",
    "                    sentiment_label = 0  \n",
    "                else:\n",
    "                    sentiment_label = 1  \n",
    "\n",
    "                Data[idx] = {\n",
    "                    'comment': comment.replace(\" \", \"\"),  \n",
    "                    'themes': multi_hot_vector,  \n",
    "                    'sentiment': sentiment_label  \n",
    "                }\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "train_data = ChnSentiCorp('data/train.txt')\n",
    "test_data = ChnSentiCorp('data/test.txt')\n",
    "\n",
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch_samples):\n",
    "    batch_sentences, batch_themes_labels, batch_sentiment_labels = [], [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        batch_sentences.append(sample['comment'])\n",
    "        batch_themes_labels.append(sample['themes'])\n",
    "        #batch_sentiment_labels.append(int(sample['sentiment']))\n",
    "\n",
    "    batch_inputs = tokenizer(\n",
    "        batch_sentences,\n",
    "        max_length=max_length,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'batch_inputs': batch_inputs,\n",
    "        'theme_labels': batch_themes_labels,\n",
    "        #'sentiment_labels': batch_sentiment_labels\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMultiTaskLearning: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiTaskLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiTaskLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiTaskLearning were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['theme_classifier.weight', 'theme_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class BertForMultiTaskLearning(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_themes):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.num_themes = num_themes\n",
    "        #self.num_sentiments = 3\n",
    "\n",
    "        self.theme_classifier = nn.Linear(self.bert.config.hidden_size, num_themes)  # 多标签主题分类\n",
    "        #self.sentiment_classifier = nn.Linear(self.bert.config.hidden_size, num_sentiments)  # 情感分类\n",
    "\n",
    "        self.theme_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        #self.sentiment_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, batch_inputs, theme_labels=None):\n",
    "        _ = self.bert(**batch_inputs)\n",
    "\n",
    "        theme_logits = self.theme_classifier(_[1])   # [batch_size, hidden_size]\n",
    "\n",
    "        theme_loss = self.theme_loss_fn(theme_logits, theme_labels.float())\n",
    "\n",
    "        # # 情感分类\n",
    "        # sentiment_logits = self.sentiment_classifier(pooled_output)  # [batch_size, num_sentiments]\n",
    "        # sentiment_loss = None\n",
    "        # if sentiment_labels is not None:\n",
    "        #     sentiment_loss = self.sentiment_loss_fn(sentiment_logits, sentiment_labels.long())\n",
    "\n",
    "        # 返回损失和 logits\n",
    "        return {\n",
    "            \"theme_loss\": theme_loss,\n",
    "            \"theme_logits\": theme_logits,\n",
    "            # \"sentiment_loss\": sentiment_loss\n",
    "            #\"sentiment_logits\": sentiment_logits,\n",
    "        }\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForMultiTaskLearning.from_pretrained(checkpoint, config=config, num_themes=len(themes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_train_step = 0\n",
    "total_train_loss = 0.\n",
    "best_f1_score = 0.\n",
    "total_test_loss = 0\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_train_loss, total_train_step):\n",
    "    progress_bar = tqdm(range(len(dataloader)),disable=True)\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_step_num = epoch * len(dataloader)\n",
    "    true_labels, predictions = [], []\n",
    "    model.train()\n",
    "    for step, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = {k: torch.tensor(v).to(device) if isinstance(v, list) else v.to(device) for k, v in batch_data.items()}\n",
    "        theme_labels = batch_data[\"theme_labels\"]\n",
    "        theme_outputs = model(**batch_data)\n",
    "        loss = theme_outputs[\"theme_loss\"]\n",
    "        logits = theme_outputs[\"theme_logits\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        pred = predict(logits)\n",
    "        true_labels += theme_labels.cpu().numpy().tolist()\n",
    "        predictions += pred.cpu().numpy().tolist()\n",
    "        #theme_metrics = classification_report(true_labels, predictions, target_names=themes, output_dict=True)\n",
    "\n",
    "\n",
    "        total_train_step +=1\n",
    "        progress_bar.set_description(f'loss: {total_train_loss / (finish_step_num + step):>7f}')\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        if total_train_step % 100 == 0:\n",
    "            accuracy = Ac(true_labels, predictions)\n",
    "            print(\"训练次数:{}，loss:{}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "            writer.add_scalar(\"train_accuracy\", accuracy, total_train_step)\n",
    "\n",
    "    return total_train_loss, total_train_step\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, epoch):\n",
    "    true_labels, predictions = [], []\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch_data in enumerate(dataloader, start=1):\n",
    "            batch_data = {k: torch.tensor(v).to(device) if isinstance(v, list) else v.to(device) for k, v in batch_data.items()}\n",
    "            theme_labels = batch_data[\"theme_labels\"]\n",
    "            theme_outputs = model(**batch_data)\n",
    "            loss = theme_outputs[\"theme_loss\"]\n",
    "            logits = theme_outputs[\"theme_logits\"]\n",
    "\n",
    "            pred = predict(logits)\n",
    "\n",
    "            true_labels += theme_labels.cpu().numpy().tolist()\n",
    "            predictions += pred.cpu().numpy().tolist()\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    " \n",
    "    metrics = classification_report(true_labels, predictions, target_names=themes, output_dict=True)\n",
    "\n",
    "    macro_precision = metrics[\"macro avg\"][\"precision\"]\n",
    "    macro_recall = metrics[\"macro avg\"][\"recall\"]\n",
    "    macro_f1 = metrics['macro avg']['f1-score']\n",
    "\n",
    "    accuracy = Ac(true_labels, predictions)\n",
    "\n",
    "    print(\"整体测试集上的Loss:{}\".format(total_test_loss))\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, epoch)\n",
    "    writer.add_scalar(\"test_accuarcy\", accuracy, epoch)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:>0.2f}\\n\")\n",
    "    print(f\"Recall: {macro_recall * 100:>0.2f}\\n\")\n",
    "    print(f\"Precision: {macro_precision * 100:>0.2f}\\n\")\n",
    "    print(f\"Macro-F1: {macro_f1 * 100:>0.2f}\\n\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num * len(train_dataloader),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "------------------------------\n",
      "训练次数:100，loss:0.14128893613815308\n",
      "训练次数:200，loss:0.238071471452713\n",
      "训练次数:300，loss:0.05874832347035408\n",
      "训练次数:400，loss:0.14770178496837616\n",
      "训练次数:500，loss:0.14221671223640442\n",
      "训练次数:600，loss:0.10402069240808487\n",
      "训练次数:700，loss:0.1474270224571228\n",
      "训练次数:800，loss:0.09061691164970398\n",
      "训练次数:900，loss:0.025390852242708206\n",
      "训练次数:1000，loss:0.05570119246840477\n",
      "训练次数:1100，loss:0.08061929792165756\n",
      "训练次数:1200，loss:0.09871605783700943\n",
      "训练次数:1300，loss:0.041693590581417084\n",
      "训练次数:1400，loss:0.0485195517539978\n",
      "训练次数:1500，loss:0.03534787520766258\n",
      "训练次数:1600，loss:0.07674387842416763\n",
      "训练次数:1700，loss:0.02461278811097145\n",
      "训练次数:1800，loss:0.031014425680041313\n",
      "训练次数:1900，loss:0.021289953961968422\n",
      "训练次数:2000，loss:0.02409210614860058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体测试集上的Loss:50.0447001317516\n",
      "Accuracy: 82.81\n",
      "\n",
      "Recall: 90.39\n",
      "\n",
      "Precision: 86.14\n",
      "\n",
      "Macro-F1: 88.01\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/3\n",
      "------------------------------\n",
      "训练次数:2100，loss:0.0386396162211895\n",
      "训练次数:2200，loss:0.04403633996844292\n",
      "训练次数:2300，loss:0.009903945960104465\n",
      "训练次数:2400，loss:0.02262192964553833\n",
      "训练次数:2500，loss:0.10436846315860748\n",
      "训练次数:2600，loss:0.15481458604335785\n",
      "训练次数:2700，loss:0.020712612196803093\n",
      "训练次数:2800，loss:0.025577455759048462\n",
      "训练次数:2900，loss:0.04280760884284973\n",
      "训练次数:3000，loss:0.025423675775527954\n",
      "训练次数:3100，loss:0.20209431648254395\n",
      "训练次数:3200，loss:0.14644818007946014\n",
      "训练次数:3300，loss:0.050151217728853226\n",
      "训练次数:3400，loss:0.06498054414987564\n",
      "训练次数:3500，loss:0.10896965116262436\n",
      "训练次数:3600，loss:0.07368852198123932\n",
      "训练次数:3700，loss:0.02990954928100109\n",
      "训练次数:3800，loss:0.025648152455687523\n",
      "训练次数:3900，loss:0.13362044095993042\n",
      "训练次数:4000，loss:0.1389678567647934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体测试集上的Loss:45.46855849120766\n",
      "Accuracy: 83.38\n",
      "\n",
      "Recall: 89.96\n",
      "\n",
      "Precision: 86.88\n",
      "\n",
      "Macro-F1: 88.33\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 3/3\n",
      "------------------------------\n",
      "训练次数:4100，loss:0.11974555253982544\n",
      "训练次数:4200，loss:0.017473747953772545\n",
      "训练次数:4300，loss:0.08850589394569397\n",
      "训练次数:4400，loss:0.015337551943957806\n",
      "训练次数:4500，loss:0.028903687372803688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of <tqdm.notebook.tqdm_notebook object at 0x0000016EDD010240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\tqdm\\std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\tqdm\\notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "  File \"C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\tqdm\\notebook.py\", line 160, in display\n",
      "    d = self.format_dict\n",
      "  File \"C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\tqdm\\std.py\", line 1476, in format_dict\n",
      "    'colour': self.colour}\n",
      "  File \"C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\tqdm\\notebook.py\", line 204, in colour\n",
      "    return self.container.children[-2].style.bar_color\n",
      "AttributeError: 'FloatProgress' object has no attribute 'style'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数:4600，loss:0.015385778620839119\n",
      "训练次数:4700，loss:0.044226642698049545\n",
      "训练次数:4800，loss:0.04595111310482025\n",
      "训练次数:4900，loss:0.02217872627079487\n",
      "训练次数:5000，loss:0.14120233058929443\n",
      "训练次数:5100，loss:0.09069563448429108\n",
      "训练次数:5200，loss:0.012033330276608467\n",
      "训练次数:5300，loss:0.010209862142801285\n",
      "训练次数:5400，loss:0.13968037068843842\n",
      "训练次数:5500，loss:0.07125606387853622\n",
      "训练次数:5600，loss:0.011379818432033062\n",
      "训练次数:5700，loss:0.08518587052822113\n",
      "训练次数:5800，loss:0.09954748302698135\n",
      "训练次数:5900，loss:0.055016547441482544\n",
      "训练次数:6000，loss:0.0365290492773056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17576\\anaconda3\\envs\\yl\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体测试集上的Loss:45.55600940622389\n",
      "Accuracy: 83.53\n",
      "\n",
      "Recall: 90.51\n",
      "\n",
      "Precision: 87.07\n",
      "\n",
      "Macro-F1: 88.66\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='themes_classification_logs' + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch {epoch + 1}/{epoch_num}\\n\" + 30 * \"-\")\n",
    "    total_train_loss, total_train_step= train_loop(train_dataloader, model, optimizer, lr_scheduler, epoch, total_train_loss, total_train_step)\n",
    "    valid_scores = test_loop(test_dataloader, model, epoch)\n",
    "    macro_f1, micro_f1 = valid_scores['macro avg']['f1-score'], valid_scores['weighted avg']['f1-score']\n",
    "    f1_score = (macro_f1 + micro_f1) / 2\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f'epoch_{epoch + 1}_valid_macrof1_{(macro_f1 * 100):0.3f}_microf1_{(micro_f1 * 100):0.3f}_model_weights.bin'\n",
    "        )\n",
    "\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
